{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "This is a hands on experience building a Deep Convolutional Generative Adversarial Network (DCGAN). The following implementation is based on the [original paper](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "More details about this notebook as well as a quick introduction to GANs can be found in the accompanied article [here](https://sthalles.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gzip\n",
    "import zipfile\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "This implementation is built to support two datasets, [The Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) and the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. To choose between them, just assign one of the option variables to the **dataset_name** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MNIST_DATASET = 'mnist'\n",
    "SVHN_DATASET = 'svhn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = MNIST_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal here is to focus on GANs specifically, the details of downloading a preprocessing the dataset is factored in this `Dataset()` class from the `utils` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.Dataset(dataset_name, shuffle=True)\n",
    "print(\"Dataset shape:\", dataset.images().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm showing a small sample of the images. Each of these is 32x32 with 3 color channels (RGB), for the SVHN and 32x32x1 for the MNIST images. Note that for the MNIST dataset we opted to pad the 28x28 black and white images with 0s so that they match the SVHNs spatial dimensions. These are the real images we'll pass to the discriminator and what the generator will eventually learn to fake.\n",
    "\n",
    "This dataset object already does the required preprocessing, i.e. scale the images between -1 and 1 and it also has a `next_batch()` method for getting training mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(dataset, figsize=(6,6), denomalize=False):\n",
    "    fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=figsize,)\n",
    "    for ii, ax in enumerate(axes.flatten()):\n",
    "        img = dataset[ii,:,:,:]\n",
    "        if dataset_name == SVHN_DATASET:\n",
    "            if denomalize:\n",
    "                img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8) # Scale back to 0-255\n",
    "            ax.imshow(img, aspect='equal')\n",
    "        elif dataset_name == MNIST_DATASET:\n",
    "            if denomalize:\n",
    "                img = (img - img.min()) / (img.max() - img.min()) # Scale back to 0-1\n",
    "            ax.imshow(img.reshape(32,32), cmap='gray')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(dataset.images())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions\n",
    "\n",
    "These are Tensorflow's wrapper function for the most important routines we will be using in this implementation. According to the paper, the variables are initialized with values from a normal distribution with *mean* of 0 and *standard deviation* of 0.02. Both convolutions and transpose convolutions have 'same' padding and they both use strides of 2 either to reduce in half or to double increase the inputs’ spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(x, out_units):\n",
    "    return tf.layers.dense(x, out_units, activation=None,\n",
    "                          kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02))\n",
    "\n",
    "def conv2d(x, output_space):\n",
    "    return tf.layers.conv2d(x, output_space, kernel_size=5, strides=2, padding=\"same\", activation=None,\n",
    "                           kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02))\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "     # non-linear activation function\n",
    "    return tf.maximum(alpha * x, x)\n",
    "\n",
    "def batch_norm(x, training, epsilon=1e-5, momentum=0.9):\n",
    "     return tf.layers.batch_normalization(x, training=training, epsilon=epsilon, momentum=momentum)\n",
    "    \n",
    "def transpose_conv2d(x, output_space):\n",
    "    return tf.layers.conv2d_transpose(x, output_space, 5, strides=2, padding='same',\n",
    "                                     kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Inputs\n",
    "\n",
    "Here, just creating some placeholders to feed the Generator and Discriminator nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    \n",
    "    return inputs_real, inputs_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The network has 4 convolutional layers, all of them followed by batch normalization (except for the output layer) and rectified linear unit (RELU) activations. It will take as input a random vector z (drawn from a normal distribution), which will be reshaped in a 4D tensor and start a series of upsampling layers by applying transpose convolutional operations with strides of 2.\n",
    "\n",
    "All the transpose convolutions use kernel filters of size 5x5 and the kernel depth goes from 512 all the way down to 3 - representing the RGB color channels. The final layer then outputs a 32x32x3 tensor that will be squashed between -1 and 1 through the [Hyperbolic Tangent](https://reference.wolfram.com/language/ref/Tanh.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True):\n",
    "    \"\"\"\n",
    "    Generator network\n",
    "    :param z: input random vector z\n",
    "    :param output_dim: output dimension of the network\n",
    "    :param reuse: Indicates whether or not the existing model variables should be used or recreated\n",
    "    :param alpha: scalar for lrelu activation function\n",
    "    :param training: Boolean for controlling the batch normalization statistics\n",
    "    :return: model's output\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        fc1 = dense(z, 4*4*512)\n",
    "        \n",
    "        # Reshape it to start the convolutional stack\n",
    "        fc1 = tf.reshape(fc1, (-1, 4, 4, 512))\n",
    "        fc1 = batch_norm(fc1, training=training)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        \n",
    "        t_conv1 = transpose_conv2d(fc1, 256)\n",
    "        t_conv1 = batch_norm(t_conv1, training=training)\n",
    "        t_conv1 = tf.nn.relu(t_conv1)\n",
    "        \n",
    "        t_conv2 = transpose_conv2d(t_conv1, 128)\n",
    "        t_conv2 = batch_norm(t_conv2, training=training)\n",
    "        t_conv2 = tf.nn.relu(t_conv2)\n",
    "        \n",
    "        logits = transpose_conv2d(t_conv2, output_dim)\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator is also a 4 layer convolutional neural network followed by batch normalization (except its input layer) and leaky RELU activations. The network receives a 32x32x3 image tensor and performs regular convolutional operations with ‘same’ padding and strides of 2 - which basically double the size of the filters at each layer. Finally, the discriminator needs to output probabilities. For that, we use the Logistic [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function for the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2, training=True):\n",
    "    \"\"\"\n",
    "    Discriminator network\n",
    "    :param x: input for network\n",
    "    :param reuse: Indicates whether or not the existing model variables should be used or recreated\n",
    "    :param alpha: scalar for lrelu activation function\n",
    "    :param training: Boolean for controlling the batch normalization statistics\n",
    "    :return: A tuple of (sigmoid probabilities, logits)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Input layer is 32x32x?\n",
    "        conv1 = conv2d(x, 64)\n",
    "        conv1 = lrelu(conv1, alpha)\n",
    "        \n",
    "        conv2 = conv2d(conv1, 128)\n",
    "        conv2 = batch_norm(conv2, training=training)\n",
    "        conv2 = lrelu(conv2, alpha)\n",
    "        \n",
    "        conv3 = conv2d(conv2, 256)\n",
    "        conv3 = batch_norm(conv3, training=training)\n",
    "        conv3 = lrelu(conv3, alpha)\n",
    "\n",
    "        # Flatten it\n",
    "        flat = tf.reshape(conv3, (-1, 4*4*256))\n",
    "        logits = dense(flat, 1)\n",
    "\n",
    "        out = tf.sigmoid(logits)\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loss\n",
    "\n",
    "We know that the discriminator receives images from both, the training set and from the generator. We want the discriminator to be able to distinguish between real and fake images. Since we want the discriminator to output probabilities close to 1 for real images and near 0 for fake images, we need two partial losses for the discriminator. The total loss for the discriminator is then, the sum of the two losses - one for maximizing the probabilities for the real images and another for minimizing the probability of fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, alpha=0.2, smooth=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: random vector z\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :param smooth: label smothing scalar \n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    g_model = generator(input_z, output_dim, alpha=alpha)\n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "    tf.summary.scalar('mean_discriminator_output_prob_real', tf.reduce_mean(d_model_real))\n",
    "        \n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, alpha=alpha)\n",
    "    tf.summary.scalar('mean_discriminator_output_prob_fake', tf.reduce_mean(d_model_fake))\n",
    "    \n",
    "    # for the real image from the training set, we want them to be classified as positives,  \n",
    "    # so we want their labels to be all ones. \n",
    "    # notice here we use label smoothing for helping the discriminator to generalize better. \n",
    "    # Label smoothing works by avoiding the classifier to make extreme predictions when extrapolating.\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "    \n",
    "    # for the fake images produced by the generator, we want the discriminator to clissify them as false images,\n",
    "    # so we set their labels to be all zeros.\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    # since the generator wants the discriminator to output 1s for its images, it uses the discriminator logits for the\n",
    "    # fake images and assign labels of 1s to them.\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\n",
    "    tf.summary.scalar('generator_loss', g_loss)\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    tf.summary.scalar('discriminator_loss', d_loss)\n",
    "    \n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Because the generator and the discriminator networks train simultaneity, GANs require two optimizers to run at the same time. Each one for minimizing the discriminator and generator’s loss functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_optimizers(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias variables for each network\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Because the batch norm layers are not part of the graph we inforce these operation to run before the \n",
    "    # optimizers so the batch normalization layers can update their population statistics.\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_size = dataset.images().shape[1:]\n",
    "z_size = 100\n",
    "learning_rate = 0.0002\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "alpha = 0.2\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the placeholders from the helper functions and setup other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_real, input_z = model_inputs(real_size, z_size)\n",
    "d_loss, g_loss = model_loss(input_real, input_z, real_size[2], alpha=0.2)\n",
    "d_opt, g_opt = model_optimizers(d_loss, g_loss, learning_rate, 0.5)\n",
    "\n",
    "sample_z = np.random.uniform(-1, 1, size=(36, z_size))\n",
    "\n",
    "image_counter = 0\n",
    "\n",
    "# change this variable if you want to produce video with the generator's samples during training\n",
    "save_video = False\n",
    "\n",
    "if save_video:\n",
    "    folder = \"./video\"\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "steps = 0\n",
    "with tf.Session() as sess:\n",
    "    # Merge all the summaries and write them out\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./logdir/' + str(time.time()), sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x in dataset.next_batch(batch_size):\n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "\n",
    "            # Update the discriminator network\n",
    "            _, summary, train_loss_d = sess.run([d_opt, merged, d_loss], feed_dict={input_real: x, input_z: batch_z})\n",
    "            \n",
    "            # Update the generator twice two avoid the rapid convergence of the discriminator\n",
    "            _ = sess.run(g_opt, feed_dict={input_z: batch_z, input_real: x})\n",
    "            _, train_loss_g = sess.run([g_opt, g_loss], feed_dict={input_z: batch_z, input_real: x})\n",
    "\n",
    "            if steps % 10 == 0:\n",
    "                train_writer.add_summary(summary, steps)\n",
    "\n",
    "                print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "                      \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                      \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "            \n",
    "            if steps % 100 == 0:\n",
    "                # At the end of each batch, sample some data from the generator, display and save it.\n",
    "                # Notice when the generator creates the samples to displaied, we set training to False. \n",
    "                # That is important for signalling the batch normalization layers to use the population statistics rather \n",
    "                # than the batch statistics\n",
    "                gen_samples = sess.run(generator(input_z, real_size[2], reuse=True, training=False),\n",
    "                                       feed_dict={input_z: sample_z})\n",
    "  \n",
    "                display_images(gen_samples, denomalize=True)\n",
    "\n",
    "                # save the samples to disk\n",
    "                if save_video:\n",
    "                    plt.savefig(folder + \"/file%02d.png\" % image_counter)\n",
    "                    image_counter += 1\n",
    "                    plt.show()\n",
    "                \n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_video:\n",
    "    utils.generate_video(dataset_name, folder)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
